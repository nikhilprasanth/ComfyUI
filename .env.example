# ComfyUI Docker Environment Configuration
# Copy this file to .env and customize as needed
# Usage: docker-compose will automatically load .env file

# ============================================
# Port Configuration
# ============================================
# Port to expose ComfyUI on
# COMFYUI_PORT=8188

# ============================================
# Path Configuration
# ============================================
# Configure custom paths for models, input, and output
# Uncomment and modify to use external directories

# Models directory (contains checkpoints, VAE, LoRAs, etc.)
# Example for Windows WSL2: /mnt/d/AI/models
# Example for Linux: /home/user/AI/models
# COMFYUI_MODELS_PATH=/data/models

# Input directory (for input images)
# COMFYUI_INPUT_PATH=/data/input

# Output directory (for generated images)
# COMFYUI_OUTPUT_PATH=/data/output

# ============================================
# Performance Configuration
# ============================================
# VRAM mode: highvram, normalvram, lowvram, novram, cpu
# - highvram: Keep everything in VRAM (12GB+ recommended)
# - normalvram: Default balanced mode
# - lowvram: For GPUs with 6GB or less VRAM
# - novram: Minimal VRAM usage
# - cpu: Run on CPU only (very slow)
# COMFYUI_VRAM_MODE=normalvram

# Preview method: none, auto, latent2rgb, taesd
# - none: No previews (fastest)
# - auto: Automatic selection
# - taesd: High quality previews (requires taesd models)
# COMFYUI_PREVIEW_METHOD=auto

# ============================================
# Advanced Configuration
# ============================================
# Additional ComfyUI command-line arguments
# Examples:
#   --fast                    Enable fast mode optimizations
#   --fp16-vae               Use FP16 for VAE (may cause black images)
#   --highvram               Keep models in VRAM
#   --use-pytorch-cross-attention  Use PyTorch's cross attention
# COMFYUI_EXTRA_ARGS=

# ============================================
# GPU Configuration
# ============================================
# Visible GPU devices (default: all)
# Example: Use only GPU 0: NVIDIA_VISIBLE_DEVICES=0
# NVIDIA_VISIBLE_DEVICES=all

# ============================================
# Docker Configuration
# ============================================
# Shared memory size for large models (default: 8gb)
# Increase if you encounter memory issues
# SHM_SIZE=8gb
